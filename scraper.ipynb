{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178f316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cfscrape\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d41ed3",
   "metadata": {},
   "source": [
    "I have made a function below to specifically scrape out the description for a given job url id from LinkedIn - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af9e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linkedin_job_description(job_id):\n",
    "    job_url = f'https://www.linkedin.com/jobs/view/{job_id}/'\n",
    "    scraper = cfscrape.create_scraper()\n",
    "    response = scraper.get(job_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        description_div = soup.find('div', class_='description__text description__text--rich')\n",
    "        job_description = description_div.get_text(strip=True) if description_div else \"No job description found\"\n",
    "        return job_description\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c728169",
   "metadata": {},
   "source": [
    "Trying the function on a list of job url ids for 5 Data Engineer roles -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4136c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_engineer_id_list = ['3679604075','3686066707','3747655980','3752887099','3765019895']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e628ae02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineer:We are looking for a junior/mid-level Data Engineer to join our data engineering team! For this role, you will be working on large SSIS/SSRS projects that will expand to have business intelligence analytics. There will be great potential to learn various database engineering skills in this position- ranging from SQL to analytics. You should be comfortable in a collaborative team-based environment. Must be driven to improve and expand your knowledge and technological skillset continuously. The ideal candidate should have exposure to SSRS.Requirements and Skills:Minimum of one year of Transact SQL in the form of direct queries, stored procedures, views, and/or triggersMust be comfortable working with joins, indexes as well as complex queriesMinimum of one year of SSIS experienceHands-on experience building packages using Visual Studio 2019/2022Exposure to SSRSProblem-solving attitudeStrong communication skillsWillingness to learnAbout ExponentHR:ExponentHR is a mid-sized family business located in the North Dallas area that provides its clients a human capital management platform to assist with the management of their day-to-day HR, benefits, and payroll functions. We offer our full-time employees competitive paid leave and benefit plans including medical, dental, vision, basic life insurance, and 401(k) with an employer match. ExponentHR is an equal opportunity employer committed to being an inclusive work environment and does not discriminate upon the bases of race, national origin, gender, gender identity, disability, age, sexual orientation, protected veteran status, or other legally protected statuses.Location:Our office is in North Dallas. This position is HYBRID remote; applicants should live or be willing to relocate to the Dallas-Fort Worth area. Typical in-office presence is up to 3 days a week.**Willing to sponsor for the right candidate**﻿***Please submit your resume to careers@exponenthr.com***Show moreShow less \n",
      "\n",
      "\n",
      "Company DescriptionWe help the world see new possibilities and inspire change for better tomorrows. Our analytic solutions bridge content, data, and analytics to help business, people, and society become stronger, more resilient, and sustainable.Job DescriptionWe are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data pipeline architecture. The ideal candidate is an experienced data pipeline builder and data wrangler with strong experience in handling data at scale. The Data Engineer will support our software developers, data analysts and data scientists on various data initiatives.This role is based in our Jersey City, NJ global headquarters which has a flexible hybrid work model with 2 or 3 days per week in-office and 2 or 3 days per week remote.Why this roleThis is a highly visible role within the enterprise data lake team. Working within our Data group and business analysts, you will be responsible for leading creation of data architecture that produces our data assets to enable our data platform. This role requires working closely with business leaders, architects, engineers, data scientists and wide range of stakeholders throughout the organization to build and execute our strategic data architecture vision.Job DutiesExtensive understanding of SQL queries. Ability to fine tune queries based on various RDBMS performance parameters such as indexes, partitioning, Explain plans and cost optimizers.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies stackBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Working with data scientists and industry leaders to understand data needs and design appropriate data models.Participate in the design and development of the AWS-based data platform and data analytics.QualificationsSkills NeededDesign and implement data ETL frameworks for secured Data Lake, creating and maintaining an optimal pipeline architecture.Examine complex data to optimize the efficiency and quality of the data being collected, resolve data quality problems, and collaborate with database developers to improve systems and database designsHands-on building data applications using AWS Glue, Lake Formation, Athena, AWS Batch, AWS Lambda, Python, Linux shell & Batch scripting.Hands on experience with AWS Database services (Redshift, RDS, DynamoDB, Aurora etc.)Experience in writing advanced SQL scripts involving self joins, windows function, correlated subqueries, CTE’s etc.An understanding of data management fundamentals, including concepts such as data dictionaries, data models, validation, and reporting.Education and TrainingMinimum of 5 years full-time software engineering experience with at least 2 years in an AWS environment focused on application development.Bachelor’s degree or foreign equivalent degree in Computer Science, Software Engineering, or related fieldAdditional InformationIn 2022, Verisk received Great Place to Work® Certification for our outstanding workplace culture for the sixth year in a row and second-time certification in the UK, Spain, and India. We’re also one of the 38 companies on the UK’s Best Workplaces™ list and one of 18 companies on Spain’s Best Workplaces™ list.For over fifty years and through innovation, interpretation, and professional insight, Verisk has replaced uncertainty with precision to unlock opportunities that deliver significant and demonstrable impact. From our historic roots in risk assessment, we’ve grown to provide analytic insights that help transform industries focused on some of the world’s most critical areas. Today, the insurance industry relies on Verisk to be, and to make the world, more productive, resilient, and sustainable.Verisk works in collaboration with our customers and at the intersection of people, data, and advanced technologies. Through proprietary platformed analytics, advanced modeling, and interpretation, we deliver immediate and sustained value to our customers and through them, to the individuals and societies they serve, with greater speed, precision, and scale.We’re 9,000 people strong, committed to translating big data into big ideas. We help others see new possibilities and empower certainty into big decisions that impact individuals and societies. And we relentlessly and ethically pursue innovation to help move our customers, and the world, toward better tomorrows.Everyone at Verisk—from our chief executive officer to our newest employee—is guided by The Verisk Way, to Be Remarkable, Add Value, and Innovate.Be Remarkable by doing something better each day in service to our customers and each otherAdd Value by delivering immediate and sustained results that drive positive outcomesInnovate by redefining what’s possible, embracing challenges, and pushing boundariesVerisk BusinessesUnderwriting Solutions — provides underwriting and rating solutions for auto and property, general liability, and excess and surplus to assess and price risk with speed and precisionClaims Solutions — supports end-to-end claims handling with analytic and automation tools that streamline workflow, improve claims management, and support better customer experiencesProperty Estimating Solutions — offers property estimation software and tools for professionals in estimating all phases of building and repair to make day-to-day workflows the most efficientExtreme Event Solutions — provides risk modeling solutions to help individuals, businesses, and society become more resilient to extreme events.Specialty Business Solutions — provides an integrated suite of software for full end-to-end management of insurance and reinsurance business, helping companies manage their businesses through efficiency, flexibility, and data governanceMarketing Solutions — delivers data and insights to improve the reach, timing, relevance, and compliance of every consumer engagementLife Insurance Solutions – offers end-to-end, data insight-driven core capabilities for carriers, distribution, and direct customers across the entire policy lifecycle of life and annuities for both individual and group.Verisk Maplecroft — provides intelligence on sustainability, resilience, and ESG, helping people, business, and societies become strongerAt Verisk you can build an exciting career with meaningful work; create positive and lasting impact on business; and find the support, coaching, and training you need to advance your career. We have received the Great Place to Work® Certification for the 7th consecutive year. We’ve been recognized byForbesas a World’s Best Employer and a Best Employer for Women, testaments to our culture of engagement and the value we place on an inclusive and diverse workforce. Verisk’s Statement on Racial Equity and Diversity supports our commitment to these values and affecting positive and lasting change in the communities where we live and work.Verisk Analytics is an equal opportunity employer.All members of the Verisk Analytics family of companies are equal opportunity employers. We consider all qualified applicants for employment without regard to race, religion, color, national origin, citizenship, sex, gender identity and/or expression, sexual orientation, veteran's status, age or disability.http://www.verisk.com/careers.htmlUnsolicited resumes sent to Verisk, including unsolicited resumes sent to a Verisk business mailing address, fax machine or email address, or directly to Verisk employees, will be considered Verisk property. Verisk will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.HR CCPA Privacy Notice.pdfShow moreShow less \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sr. Data Engineer Location - Seattle, WAFull TimePlease apply Only 8+ years and H1B transfer allows and GC & Citizens (Willing to Relocate to Seattle)Need Skills ETL, Big data with (AWS OR Azure)Please share your updated resume to manikanta.potla@peopletech.comAs a Data Engineer, you will be working in one of the world's largest and most complex data Lake and warehouse environments. You will design, implement, and support scalable data infrastructure solutions to integrate with multi heterogeneous data sources, aggregate and retrieve data in a fast and safe mode, curate data that can be used in reporting, analysis, machine learning models and ad-hoc data requests. You will be exposed to cutting edge AWS big data technologies. This will be a great fit if you get excited by partnering with key stakeholders to dig deep into the business challenges to understand and identify insights that will enable us to meet associate needs at a globally scale. You need excellent analytical abilities as well as business acumen and an ability to tie together from across Amazon to help enable effective decision making. An ability to communicate insight to the business via development and implementation of ongoing and subject-specific reporting artifacts is a critical skill in this role.Key job responsibilitiesWork closely with offsite data engineers & Business Intelligence Engineer to deliver the projects on time.Clarify the requirements and guide the offsite team with all the technical details.Interface with technology teams to extract, transform, and load data from a wide variety of data sources using SQL/python and AWS big data technologies.Designing and implementing complex ETL pipelines and other BI solutions.Basic Qualification5+ years of data engineering experienceLeading teams, gathering requirementsExperience with data modeling, warehousing, and building ETL pipelines.Knowledge of distributed systems as it pertains to data storage and computing.Thanks & RegardsManikantaShow moreShow less \n",
      "\n",
      "\n",
      "Why this Role is Interesting?There are very few companies in the world with the credentials we have. You’ll have an opportunity to support critically important customer missions and work with paradigm-changing technology.You’ll be supporting one of the largest programs and customers in the company, offering significant visibility of your impact.The role is critical in positioning Torch.AI as a leader in data infrastructure AI in the market and helps ensure the company is well-positioning for the next stages of growth.You’ll have the opportunity to showcase your skills as a seasoned data engineer, while also learning from a team of extremely talented and credentialed individuals.Torch.AI is in the midst of a pivotal, high-growth period, with several initiatives and investments anticipated to fuel hyper-growth across a diverse customer base.You’ll play a significant roe in strengthening Torch.AI’s reputation as one of Forbes Best Startup Employers.A Game-Changing AI Company in the Heart of the MidwestTorch.AI, the Data Infrastructure AI Pioneers™, are headquartered in Kansas City with offices in Washington, DC. We build AI that makes data easier to use by processing data in-flight and radically evolving analytic and operational capabilities in any IT environment. Our Torch Platform instantly unlocks value from data and provides information needed for humans and machines to be more productive. Partnering with U.S. military forces, our solutions and people support a growing array of national defense capabilities with advanced technology.At Torch.AI, we’re passionate about building software that solves some of the world’s most challenging problems. We have helped our customers enhance top-secret clearances, stop fraud at massive scales, discover new trends and global events, gain an edge in financial markets, and beyond. We are driven by our mission to unlock human potential and serve our clients in valuable and meaningful ways.The RoleThe U.S. Department of Defense relies on complex data every day to drive decisions, support warfighters, mitigate risks to national security, and ensure our global leadership. Data is being generated at an exponential rate and existing solutions don’t address today’s needs.This Data Engineer role will work in support of developing and implementing solutions for a major U.S. Department of Defense program. The role will engage with cross-functional teams to discover all existing data types and documentation that are currently in use in multiple data producer environments. The successful candidate will be a creative problem solver, diligent and detail oriented, and will have a sharp focus on value creation and mission importance. They will have a keen eye on understanding customer needs, how they currently leverage Torch.AI software, and will help in driving the future success of the customer program.What Success Looks LikeParticipates in design meetings and consults with other staff to evaluate interface between hardware and software, and operational and performance requirements of overall system.Develop and perform automated builds, testing, and deployments in support of NiFi development.Work with the development and services teams to support service artifacts, which could include bug fixes, security improvements, functional extension, performance improvement, refactoring, and/or rewriting.Experience in building data ingestion workflows/pipeline flows using NiFi, NiFi registry, and other Nifi management tools.Create Artifact that will primarily be Apache Nifi flows and/or custom processors used in Nifi flows.Create tables in Trino to deliver data via APIs to package and disseminate data to mission partners.Build data models and analytics to support mission needs.Pull or receive command and control files.Test services artifacts for correctness and/or performance.What We ValueB.S. degree in related field or equivalent combination of training and experience.5+ years of proven work experience as a data engineer.Demonstrable experience with Java, Parquet, Nifi, Kafka.Experience with ETLs and APIs.Understanding and Experience with Apache Nifi tool.Analysis of the product to determine the end points as per the requirements.Well-versed with docker.Hands on experience on GIT.Understanding of security products focusing on IAM.Knowledge on ETL focusing on mapping.Good to have experience with NiFi-supported scripting languages (Python) and writing regular expressions.Experience creating custom NiFi processors.Full Software Development Life Cycle (SDLC) experience.Work well within a formal team structure and with minimal supervision.This position REQUIRES an active TS/SCI. TS/SCI with CI Poly is preferred.Work Environment & Travel RequirementsThis job operates in state-of-the-art, professional office environment.You will be expected to work out of Torch.AI's Leawood, KS office; limited hybrid/remote days may occasionally be available.Travel: You will be required to travel to Northern Virginia and/or St. Louis on a periodic basis (10-50%) to work in and attend client meetings in a SCIF environment.Perks & CompensationCompetitive salary, performance bonus, and benefits package. The salary range for this role is commensurate with experience.Opportunity to participate in Torch.AI’s employee equity grant program.Unlimited PTO.In-Office Catering for Lunch Every Monday.Access to company suite at the T-Mobile Center, with tickets to all major events and concerts.Amazing professional growth opportunity at a high growth start-up.Passionate, smart, and fun people to work with.Excellent medical, dental, and vision insurance.Life and disability coverage.Relocation assistance.11 paid holidays each year.Torch.AI is an Equal Opportunity /Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, protected veteran status or status as an individual with a disability.Show moreShow less \n",
      "\n",
      "\n",
      "Job Title: Python/ Data EngineerLocation: Wilmington, DE/ Plano TX (Day1 Onsite)Job Type: Open for both Contract & Fulltime JobJob Description:•            10+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.•            5+ years of experience migrating/developing data solutions in the AWS cloud is required.•            2+ years of experience building/implementing data pipelines using Databricks or similar cloud database.•            Expert level knowledge of using SQL to write complex, highly optimized queries across large volumes of data.•            Hands-on object-oriented programming experience using Python is required.•            Professional work experience building real-time data streams using Spark and Experience in Spark.•            Knowledge or experience in architectural best practices in building data lakesShow moreShow less \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for job_id in data_engineer_id_list:\n",
    "    print(get_linkedin_job_description(job_id),'\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
